# 주요 작업 내용

# Okt tokenizer 토큰화 튜닝

도메인이 논문 내용이기 때문에 긴 길이의 단어가 많이 나오고 띄어쓰기가 일반적인 한국어 문장과 차이가 있어 형태소 Tokenizer의 성능이 별로 좋지 못 했고,
키워드를 추출하기 위해서 명사 품사를 따로 추출하였기 때문에 원문의 손실이 발생하였기 때문에 그 부분을 극복하기 위해서 다양한 예외처리를 해줌 

키워드 추출의 장점
상대적으로 분류 Task에 의미가 있는 단어들만을 사용하기 때문에 동사, 조사, 형용사 등을 제외하여 상대적으로 같은 길이 내에서 더 많은 키워드를 사용할 수 있고,
이는 보다 긴 인풋을 사용할 수 있다는 의미이기도 함.
서브워드 임베딩을 샤용하더라도 띄어쓰기가 통상적인 한국어와 차이가 있기 때문에 형태소 토크나이저를 사용하는 것이 보다 효율적인 토큰 구성을 도와줄 수 있음

키워드 추출의 단점 
형태소 토크나이저의 성능에 따라 원문에서 손실되는 내용이 좌우됨. 토크나이저의 성능에 한계로 발생하는 손실을 얼마나 제어해줄 수 있느냐가 중요해짐
서브워드 임베딩을 사용할 경우 이번 데이터와 같이 형태소 토크나이저를 아예 사용하지 않는 것이 시간과 노력을 줄일 수 있음.

# Focal Loss 사용

Focal Loss는 원래 CV분야에서 전체 이미지 중 타겟의 비율이 극단적으로 낮은 경우를 해결하기 위해 나온 Loss로 데이터가 불균형할 경우에 좋은 성능을 보여줌.
일반적인 크로스엔트로피를 사용하는 것보다 Focal Loss를 사용할 때 더 좋은 성능을 보여줌

# 1D CNN 모델 사용

기본적으로 CNN 모델은 다른 사전학습 모델들에 비해 성능이 떨어진다는 인식이 많이 있지만 이번 분류 Task에서는 1D CNN이 단일 모델로는 상당히 준수한 성능을 보여주고,
Bert로 테스트를 했을 때보다 더 좋은 성능을 보여줌.

사용 컬럼 별로 길이가 많이 차이가 났기 때문에 상대적으로 긴 인풋을 사용하는 하는 부분을 분리하여 2개의 input을 받는 모델을 사용
